{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda  uninstall tensorboard; pip uninstall -y tensorboard; conda install tensorboard; conda install pytorch-lightning -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_item 131263\n",
      "n_user 138494\n",
      "n_features 269757\n",
      "n_rows 19950567\n"
     ]
    }
   ],
   "source": [
    "!python ../src/download_ml20.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "fh = np.load('data/dataset_ml20_wide.npz')\n",
    "# We have a bunch of feature columns and last column is the y-target\n",
    "max_seq_len = 768 + 1\n",
    "train_items = fh['train_items'].astype(np.int64)[:, :max_seq_len]\n",
    "# Note that ratings are modified are on scale (1, 2, ... 10) \n",
    "train_ratng = fh['train_ratng'].astype(np.int64)[:, :max_seq_len]\n",
    "test_items = fh['test_items'].astype(np.int64)[:, :max_seq_len]\n",
    "test_ratng = fh['test_ratng'].astype(np.int64)[:, :max_seq_len]\n",
    "\n",
    "n_user = train_items.shape[0]\n",
    "n_rank = train_items.shape[1]\n",
    "n_item = int(train_items.max() + 1)\n",
    "n_resp = int(train_ratng.max() + 1)\n",
    "\n",
    "train_items, val_items, train_ratng, val_ratng = train_test_split(train_items, train_ratng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import from_numpy\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "def dataloader(*arrs, batch_size=64):\n",
    "    dataset = TensorDataset(*arrs)\n",
    "    arr_size = len(arrs[0])\n",
    "    bs = BatchSampler(SequentialSampler(range(arr_size)),\n",
    "                      batch_size=batch_size, drop_last=False)\n",
    "    return DataLoader(dataset, batch_sampler=bs, shuffle=False)\n",
    " \n",
    "train = dataloader(from_numpy(train_items), from_numpy(train_ratng))\n",
    "val = dataloader(from_numpy(val_items), from_numpy(val_ratng))\n",
    "test = dataloader(from_numpy(test_items), from_numpy(test_ratng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the inputs are now 2D. Each row in `train_items` represents is a 1D stream of items seen by a single user. Different rows will be from different user streams. Note that each stream is padded with zeros so it is a fixed input size. `train_ratng` is a similar structure, but gives the categorical rating output (scaled from [0.0, 0.5, ... 4.5, 5.0] to [0, 1,2, ...10]) that that user gave that item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5, 1777,  158, ...,    0,    0,    0],\n",
       "       [  25,   95,  141, ...,    0,    0,    0],\n",
       "       [ 150,  296,  380, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [2021, 2193,   29, ...,    0,    0,    0],\n",
       "       [1210, 1291, 1342, ...,    0,    0,    0],\n",
       "       [2761, 2762, 2858, ..., 2474, 2745, 2757]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  8,  1, ...,  0,  0,  0],\n",
       "       [ 2,  8,  8, ...,  0,  0,  0],\n",
       "       [ 8,  8,  8, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 8, 10,  8, ...,  0,  0,  0],\n",
       "       [ 6, 10,  6, ...,  0,  0,  0],\n",
       "       [ 8,  4, 10, ...,  6,  8,  8]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_ratng[train_items > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      2,      3, ..., 131158, 131162, 131237])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((93483, 769), (13849, 769))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_items.shape, test_items.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q reformer_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch import from_numpy\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from reformer_pytorch import Reformer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstract_model import AbstractModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR(AbstractModel):\n",
    "    def __init__(self, n_item, n_dim, n_resp, n_rank, p=0.1,\n",
    "                 heads=2, depth=2, batch_size=32, weight_decay=1e-6):\n",
    "        super().__init__()\n",
    "        self.n_dim = n_dim\n",
    "        self.n_item = n_item\n",
    "        self.n_resp = n_resp\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # This means that item=0 will always yield the zero vector\n",
    "        self.item = nn.Embedding(n_item, n_dim, padding_idx=0)\n",
    "        self.resp = nn.Embedding(n_resp, n_dim)\n",
    "        self.reformer = Reformer(dim=n_dim, depth=depth, heads=heads, causal=True, max_seq_len=n_rank)\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "    \n",
    "    def forward(self, items, ratng):\n",
    "        item_vec = self.item(items)\n",
    "        resp_vec = self.resp(ratng)\n",
    "        intx_vec = self.dropout(item_vec * resp_vec)\n",
    "        mask = items != 0\n",
    "        user_vec = self.reformer(intx_vec, input_mask=mask)\n",
    "        return user_vec\n",
    "    \n",
    "    def loss(self, user_raw, items, ratg):\n",
    "        # user_vec is (batchsize, window, n_dim)\n",
    "        batchsize, window, n_dim = user_raw.shape\n",
    "        item_raw = self.item(items)\n",
    "        user_bas, user_vec = user_raw[:, :, 0], user_raw[:, :, 1:]\n",
    "        item_bas, item_vec = item_raw[:, :, 0], item_raw[:, :, 1:]\n",
    "        pred = user_bas + item_bas + (user_vec * item_vec).sum(dim=2)\n",
    "        # Ignore ratings that are zero -- zero isn't actually possible from the\n",
    "        # user. Instead zero is empty padding that we should ignore.\n",
    "        mask = ratg != 0\n",
    "        loss_sum = F.mse_loss(pred[mask], ratg[mask] * 1.0, reduction='sum')\n",
    "        loss_mean = loss_sum / (mask.sum() * 1.0)\n",
    "        return loss_mean, {\"mse\": loss_mean}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4, \n",
    "                                weight_decay=self.weight_decay)\n",
    "\n",
    "    def step(self, batch, batch_nb, prefix='train', add_reg=True):\n",
    "        items, ratng = batch\n",
    "        # Pass in leading arrays, missing the last element\n",
    "        # (hence the [:-1]) for every user that's to  be predicted\n",
    "        user_vec = self.forward(items[:,  :-1], ratng[:, :-1])\n",
    "        # Given previous tokens, predict the next interaction\n",
    "        # hence the [1:] \n",
    "        loss, log = self.loss(user_vec, items[:, 1:],  ratng[:, 1:])\n",
    "        log[f'{prefix}_loss'] = loss\n",
    "        return {f'{prefix}_loss': loss, 'loss':loss, 'log': log}\n",
    "        \n",
    "    def reg(self):\n",
    "        # Regularize via weight decay instead of explicitly\n",
    "        return 0.0, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7768f12ab6be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = AR(n_item, n_dim, n_resp, n_rank, \n\u001b[1;32m      5\u001b[0m            heads=8, depth=6)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWandbLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"09_mf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"simple_mf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer = pl.Trainer(max_epochs=100, logger=logger,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pytorch_lightning/loggers/wandb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, save_dir, offline, id, anonymous, version, project, log_model, experiment, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     ):\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             raise ImportError('You want to use `wandb` logger which is not installed yet,'  # pragma: no-cover\n\u001b[0m\u001b[1;32m     86\u001b[0m                               ' install it with `pip install wandb`.')\n\u001b[1;32m     87\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: You want to use `wandb` logger which is not installed yet, install it with `pip install wandb`."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "n_dim = 48\n",
    "model = AR(n_item, n_dim, n_resp, n_rank, \n",
    "           heads=8, depth=6)\n",
    "logger = WandbLogger(name=\"09_mf\", project=\"simple_mf\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100, logger=logger,\n",
    "                     gpus=0, progress_bar_refresh_rate=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(model)\n",
    "results['avg_test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
