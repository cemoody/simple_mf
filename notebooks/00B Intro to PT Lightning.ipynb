{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xE2B7ylXyWq"
   },
   "source": [
    "## Using PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've  written a dozen  pytorch models you'll discover that there's a lot of common structure and a huge amount of boilerplate. It's good to understand what's going on undere the hood, but when moving to production use cases you'll want to opt for more reliable, reproducible code. PyTorch Lightning & Ignite are great libraries that abstract away these core bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip uninstall tensorboard\n",
    "    conda install tensorboard -y\n",
    "    conda install pytorch-lightning -y -c conda-forge\n",
    "    pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 9), (12500, 9), (37500, 4), (12500, 4))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "n = 50000\n",
    "# X is just a 9D normally distributed dataset\n",
    "X = np.random.normal(size=(n, 9)).astype(np.float32)\n",
    "# The prediction is a linear transformation on X\n",
    "# from 9D to 4D plus additive noise\n",
    "Y = np.random.normal(size=(n, 4)) * 1e-2 + np.dot(X, np.random.normal(size=(9, 4)))\n",
    "Y = Y.astype(np.float32)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write an abstract model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our initial model as Lightning  module:\n",
    "\n",
    "Don't be afraid of how much extra code this injects. Although it  initially looks like a ton of little class functions, it's all about being organized, deliberate, standardized and repeatable. It's not about science, it's about having good lab hygiene. \n",
    "\n",
    "- We'll move some of the iteration code into `training_step`  and `test_step`, and `test_epoch_end`.\n",
    "- Add in a `configure_optimizers` function.\n",
    "- Separate out train & test loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from torch import from_numpy\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "\n",
    "class AbstractModel(pl.LightningModule):\n",
    "    def step(self, batch, batch_nb, prefix='train', add_reg=True):\n",
    "        inpt, target = batch\n",
    "        prediction = self.forward(inpt)\n",
    "        loss = self.loss(prediction, target)\n",
    "        if add_reg:\n",
    "            loss = loss + self.reg()\n",
    "        tensorboard_logs = {f'{prefix}_loss': loss}\n",
    "        return {f'{prefix}_loss': loss, 'loss':loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        return self.step(batch, batch_nb, 'train')\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        # Note that we do *not* include the regularization / reg loss\n",
    "        # at test time\n",
    "        return self.step(batch, batch_nb, 'test', add_reg=False)    \n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        return self.step(batch, batch_nb, 'val', add_reg=False)    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        test_loss_mean = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        log = {'val_loss': test_loss_mean}\n",
    "        return {'avg_test_loss': test_loss_mean, 'log': log}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        test_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        log = {'val_loss': test_loss_mean}\n",
    "        return {'avg_val_loss': test_loss_mean, 'log': log}\n",
    "\n",
    "    def dataloader(self, is_train=True):\n",
    "        if is_train:\n",
    "            dataset = TensorDataset(*self.train_arrs)\n",
    "        else:\n",
    "            dataset = TensorDataset(*self.test_arrs)\n",
    "        bs = BatchSampler(RandomSampler(dataset), \n",
    "                          batch_size=self.batch_size, drop_last=False)\n",
    "        return DataLoader(dataset, batch_sampler=bs, num_workers=8)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.dataloader(is_train=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.dataloader(is_train=False)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.dataloader(is_train=False)\n",
    "    \n",
    "    def save_data(self, train_x, train_y, test_x, test_y, train_d=None, test_d=None):\n",
    "    if train_d is None:\n",
    "        self.train_arrs = [from_numpy(x) for x in [train_x, train_y]]\n",
    "        self.test_arrs = [from_numpy(x) for x in [test_x, test_y]]\n",
    "    else:\n",
    "        self.train_arrs = [from_numpy(x) for x in [train_x, train_y, train_d]]\n",
    "        self.test_arrs = [from_numpy(x) for x in [test_x, test_y, test_d]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write our specific model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we'll keep out `Bottleneck` model, but now it will inherit from our `AbstractModel`. Over the next few notebooks we'll keep using the `AbstractModel` class and just stick to focusing  our changes within the subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from pytorch_lightning.logging import WandbLogger\n",
    "\n",
    "\n",
    "class Bottleneck(AbstractModel):\n",
    "    def __init__(self, n_in_cols, n_out_cols, n_hidden=3, batch_size=32,\n",
    "                 lam1=1e-3, lam2=1e-3):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(n_in_cols, n_hidden)\n",
    "        self.lin2 = nn.Linear(n_hidden, n_out_cols)\n",
    "        self.batch_size = batch_size\n",
    "        # Regularization coefficients\n",
    "        self.lam1 = lam1\n",
    "        self.lam2 = lam2\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a minibatch of rows of our features\n",
    "        hidden = self.lin1(x)\n",
    "        # y is a minibatch of our predictions\n",
    "        y = self.lin2(hidden)\n",
    "        return y\n",
    "\n",
    "    def loss(self, prediction, target):\n",
    "        # This is just the mean squared error\n",
    "        return ((prediction - target)**2.0).sum()\n",
    "    \n",
    "    def reg(self):\n",
    "        # This computes our Frobenius norm over both matrices\n",
    "        # Note that we can access the Linear model's variables\n",
    "        # directly if we'd like. No tricks here!\n",
    "        loss_reg_m1 = (self.lin1.weight**2.0 * self.lam1).sum()\n",
    "        loss_reg_m2 = (self.lin2.weight**2.0 * self.lam2).sum()\n",
    "        return loss_reg_m1 + loss_reg_m2\n",
    "\n",
    "\n",
    "model = Bottleneck(9, 4, 3)\n",
    "model.save_data(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "# add a logger\n",
    "logger = WandbLogger(name=\"00_intro\", log_model=True, project=\"simple_mf\")\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"bottleneck_model\")\n",
    "\n",
    "# We could have turned on multiple GPUs here, for example\n",
    "# trainer = pl.Trainer(gpus=8, precision=16)    \n",
    "trainer = pl.Trainer(max_epochs=3, progress_bar_refresh_rate=10,\n",
    "                     reload_dataloaders_every_epoch=True,\n",
    "                     logger=logger)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test & train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train the model, the parameters and weights will all be initialized randomly. So when we evaluate the test loss, it'll be pretty bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit our model and then check the test loss again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! The test loss (~100) is much lower than it was before  ~4000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the link on wandb to see train progress. For me, that link looks like (you'll get your own link, this one shouldn't work for you.)s: \n",
    "\n",
    "Run page: https://app.wandb.ai/chrisemoody/simple_mf-notebooks/runs/2o5ofsn4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters with Optuna and Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to instal optuna:\n",
    "    \n",
    "    pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Sample parameters -- without declaring them in advance!\n",
    "    n_hid = trial.suggest_int('n_hid', 1, 10)\n",
    "    lam1 = trial.suggest_loguniform('lam1', 1e-8, 1e-3)\n",
    "    lam2 = trial.suggest_loguniform('lam2', 1e-8, 1e-3)\n",
    "    \n",
    "    model = Bottleneck(9, 4, n_hid, lam1=lam1, lam2=lam2)\n",
    "    model.save_data(X_train, Y_train, X_test, Y_test)\n",
    "    \n",
    "    logger = WandbLogger(name=\"00_intro_optimize\", log_model=True, project=\"simple_mf\")\n",
    "    logger.log_hyperparams(model.hparams)\n",
    "\n",
    "    # Note that we added early stoping  \n",
    "    trainer = pl.Trainer(max_epochs=3,\n",
    "                         reload_dataloaders_every_epoch=True,\n",
    "                         early_stop_callback=True,\n",
    "                         logger=logger)    \n",
    "    trainer.fit(model)\n",
    "    results = trainer.test(model)\n",
    "    return results['avg_test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "00 Intro PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
