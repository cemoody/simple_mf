{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iA_qC5BAv6w"
   },
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script that downloads and processes the MovieLens data.\n",
    "Uncomment it to run the download & processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ../src/download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "POjwTTneAv6y",
    "outputId": "b3acebb0-47b2-405c-eb40-5474b7aab5c2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fh = np.load('data/dataset.npz')\n",
    "\n",
    "# We have a bunch of feature columns and last column is the y-target\n",
    "# Note pytorch is finicky about need int64 types\n",
    "train_x = fh['train_x'].astype(np.int64)\n",
    "train_y = fh['train_y']\n",
    "\n",
    "# We've already split into train & test\n",
    "test_x = fh['test_x'].astype(np.int64)\n",
    "test_y = fh['test_y']\n",
    "\n",
    "\n",
    "n_user = int(fh['n_user'])\n",
    "n_item = int(fh['n_item'])\n",
    "n_occu = int(fh['n_occu'])\n",
    "n_rank = int(fh['n_ranks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstract_model import AbstractModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add this new `total_variation` regularizer. Instead of regularizing the *norm* of vector, as we frequently do with L2 regularization, we penalize the difference in subsequent elements. This is useful in temporal models: you want day 0 close to day 1, but you don't care if day0 and day 1 deviate away from zero.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation(array):\n",
    "    return torch.sum(torch.abs(array[:, :-1] - array[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "def l2_regularize(array):\n",
    "    return torch.sum(array ** 2.0)\n",
    "\n",
    "\n",
    "class MF(AbstractModel):\n",
    "    def __init__(self, n_user, n_item, n_occu, n_rank, \n",
    "                 k=18, kt=2, c_vector=1.0, c_bias=1.0,\n",
    "                 c_ut=1.0, c_temp=1.0, c_ovector=1.0,\n",
    "                 batch_size=128):\n",
    "        super().__init__()\n",
    "        # These are simple hyperparameters\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_vector = c_vector\n",
    "        self.c_ovector = c_ovector\n",
    "        self.c_bias = c_bias\n",
    "        self.batch_size = batch_size\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # NEW: regularization hyperparams\n",
    "        self.c_ut = c_ut\n",
    "        self.c_temp = c_temp\n",
    "        \n",
    "        # These are learned and fit by PyTorch\n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "        self.occu = nn.Embedding(n_occu, k)\n",
    "        \n",
    "        # NEW: temporal vectors\n",
    "        self.user_temp = nn.Embedding(n_user, kt)\n",
    "        self.temp = nn.Embedding(n_rank, kt)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # This is the most import function in this script\n",
    "        # These are the user indices, and correspond to \"u\" variable\n",
    "        user_id = inputs[:, 0]\n",
    "        # Item indices, correspond to the \"i\" variable\n",
    "        item_id = inputs[:, 1]\n",
    "        # vector user = p_u\n",
    "        vector_user = self.user(user_id)\n",
    "        # vector item = q_i\n",
    "        vector_item = self.item(item_id)\n",
    "        # this is a dot product & a user-item interaction: p_u * q_i\n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "        # occupation-item interaction\n",
    "        occu_id = inputs[:, 3]\n",
    "        vector_occu = self.occu(occu_id)\n",
    "        oi_interaction = torch.sum(vector_user * vector_occu, dim=1)\n",
    "        \n",
    "        # NEW: user-time interaction\n",
    "        rank = inputs[:, 2]\n",
    "        vector_user_temp = self.user_temp(user_id)\n",
    "        vector_temp = self.temp(rank)\n",
    "        ut_interaction = torch.sum(vector_user_temp * vector_temp, dim=1)\n",
    "        \n",
    "        prediction = ui_interaction + oi_interaction + ut_interaction + biases\n",
    "        return prediction\n",
    "\n",
    "    def likelihood(self, prediction, target):\n",
    "        # MSE error between target = R_ui and prediction = p_u * q_i\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        log = {\"mse\": loss_mse}\n",
    "        return loss_mse, log\n",
    "    \n",
    "    def prior(self):\n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        prior_occu = l2_regularize(self.occu.weight) * self.c_ovector\n",
    "        \n",
    "        # New: total variation regularization\n",
    "        prior_ut = l2_regularize(self.user_temp.weight) * self.c_ut\n",
    "        prior_tv = total_variation(self.temp.weight) * self.c_temp\n",
    "        \n",
    "        log = {\"prior_user\": prior_user, \"prior_item\": prior_item,\n",
    "               \"prior_bias_user\": prior_bias_user, \"prior_bias_item\": prior_bias_item,\n",
    "               \"prior_occu\": prior_occu, \"prior_ut\": prior_ut, \"prior_tv\": prior_tv\n",
    "              }\n",
    "\n",
    "        total = (prior_user + prior_item + prior_bias_user + prior_bias_item + prior_occu +\n",
    "                 prior_ut + prior_tv)\n",
    "        return total, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.logging import WandbLogger\n",
    "\n",
    "\n",
    "k = 6\n",
    "kt = 2\n",
    "c_bias = 1e-3\n",
    "c_vector = 1e-5\n",
    "c_ovector = 1e-8\n",
    "c_ut = 1e-5\n",
    "c_temp = 1e-5\n",
    "model = MF(n_user, n_item, n_occu, n_rank,\n",
    "           k=k, kt=kt, c_bias=c_bias, c_vector=c_vector,\n",
    "           c_ovector=c_ovector, c_ut=c_ut, c_temp=c_temp,\n",
    "           batch_size=1024)\n",
    "model.save_data(train_x, train_y, test_x, test_y)\n",
    "\n",
    "# add a logger\n",
    "logger = WandbLogger(name=\"04_mf\", project=\"simple_mf\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100, logger=logger,\n",
    "                     early_stop_callback=True,\n",
    "                     gpus=1, progress_bar_refresh_rate=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JR7UIF0vAv69"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zebJlH2LAv7D"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(model)\n",
    "mse = results['avg_test_loss']\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "01 Training a simple MF model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
