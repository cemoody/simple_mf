{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iA_qC5BAv6w"
   },
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script that downloads and processes the MovieLens data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_item 3953\n",
      "n_user 6041\n",
      "n_featuers 9994\n",
      "n_occu 21\n",
      "n_rows 1000209\n"
     ]
    }
   ],
   "source": [
    "# !python ../src/download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "POjwTTneAv6y",
    "outputId": "b3acebb0-47b2-405c-eb40-5474b7aab5c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1635 3196   31    0]\n",
      " [ 352 1041  480    4]\n",
      " [5271  318   37    0]\n",
      " ...\n",
      " [3871 2709   76    4]\n",
      " [2934 2450  174   20]\n",
      " [3217 1280   37    0]]\n",
      " \n",
      "[[5.]\n",
      " [4.]\n",
      " [3.]\n",
      " ...\n",
      " [4.]\n",
      " [1.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "fh = np.load('data/dataset.npz')\n",
    "\n",
    "# We have a bunch of feature columns and last column is the y-target\n",
    "# Note pytorch is finicky about need int64 types\n",
    "train_x = fh['train_x'].astype(np.int64)\n",
    "train_y = fh['train_y']\n",
    "\n",
    "# We've already split into train & test\n",
    "X_test = fh['test_x'].astype(np.int64)\n",
    "Y_test = fh['test_y']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y)\n",
    "\n",
    "\n",
    "n_user = int(fh['n_user'])\n",
    "n_item = int(fh['n_item'])\n",
    "\n",
    "# columns are user_id, item_id and other features \n",
    "# we won't use the 3rd and 4th columns\n",
    "print(X_train)\n",
    "print(' ')\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(X_train[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.],\n",
       "       [4.],\n",
       "       [3.],\n",
       "       ...,\n",
       "       [4.],\n",
       "       [1.],\n",
       "       [5.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import from_numpy\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "\n",
    "def dataloader(*arrs, batch_size=32):\n",
    "    dataset = TensorDataset(*arrs)\n",
    "    bs = BatchSampler(RandomSampler(dataset), \n",
    "                      batch_size=batch_size, drop_last=False)\n",
    "    return DataLoader(dataset, batch_sampler=bs, num_workers=8)\n",
    " \n",
    "train = dataloader(from_numpy(X_train), from_numpy(Y_train))\n",
    "test = dataloader(from_numpy(X_test), from_numpy(Y_test))\n",
    "val = dataloader(from_numpy(X_val), from_numpy(Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8CtjkJesAv63"
   },
   "source": [
    "### Define the MF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abstract_model import AbstractModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "def l2_regularize(array):\n",
    "    return torch.sum(array ** 2.0)\n",
    "\n",
    "\n",
    "class MF(AbstractModel):\n",
    "    def __init__(self, n_user, n_item, k=18, c_vector=1.0, batch_size=128):\n",
    "        super().__init__()\n",
    "        # These are simple hyperparameters\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_vector = c_vector\n",
    "        self.batch_size = batch_size\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # These are learned and fit by PyTorch\n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # This is the most import function in this script\n",
    "        # These are the user indices, and correspond to \"u\" variable\n",
    "        user_id = inputs[:, 0]\n",
    "        # Item indices, correspond to the \"i\" variable\n",
    "        item_id = inputs[:, 1]\n",
    "        # vector user = p_u\n",
    "        vector_user = self.user(user_id)\n",
    "        # equivalent:\n",
    "        # self.user.weight[user_id, :]\n",
    "        # vector item = q_i\n",
    "        vector_item = self.item(item_id)\n",
    "        # this is a dot product & a user-item interaction: p_u * q_i\n",
    "        # shape vector_user is (batch_size, k)\n",
    "        # vector_user * vector_item is shape (batch_size, k)\n",
    "        # sum(vector_user * vector_item is shape, dim=1) (batch_size)\n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        return ui_interaction\n",
    "    \n",
    "    def loss(self, prediction, target):\n",
    "        # MSE error between target = R_ui and prediction = p_u * q_i\n",
    "        # target is (batchsize, 1)\n",
    "        # target.squeeze (batchsize, )\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        return loss_mse, {\"mse\": loss_mse}\n",
    "    \n",
    "    def reg(self):\n",
    "        # Compute L2 reularization over user (P) and item (Q) matrices\n",
    "        reg_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        reg_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        # Add up the MSE loss + user & item regularization\n",
    "        log = {\"reg_user\": reg_user, \"reg_item\": reg_item}\n",
    "        total = reg_user + reg_item\n",
    "        return total, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, I typically tune the initial batch size to get an efficient number of datapoints observed per second. On a CPU run `htop` or `top` in another window / terminal tab to see how efficiently your CPU is being  used. If it spends a lot of time below 50% utilization, crank up  the batch size. GPUs are more sensitive, and it's difficult to keep them well fed. To measure efficiency, use `nvidia-smi`. That command only  gets you a snapshot though, so to keep it running use `watch -n 0.1 nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "batch_size = 1024\n",
    "k = 5\n",
    "c_vector = 1e-5\n",
    "model = MF(n_user, n_item, k=k, c_vector=c_vector,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "# add a logger\n",
    "logger = WandbLogger(name=\"01_mf\", project=\"simple_mf\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100, logger=logger,\n",
    "                     early_stop_callback=True,\n",
    "                     gpus=1,\n",
    "                     progress_bar_refresh_rate=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 13799\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20200925_153955-3u1uuj7l/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20200925_153955-3u1uuj7l/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   global_step 696299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mse 3.83900785446167\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reg_user 0.14615029096603394\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reg_item 0.142988920211792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 4.128147125244141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         _step 13958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      _runtime 5767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    _timestamp 1601054164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.8459229469299316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           mse █▇▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reg_user ▅▁▆██▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      reg_item ▃▁▄▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss █▇▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         _step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m01_mf\u001b[0m: \u001b[34mhttps://wandb.ai/sf-moody/simple_mf/runs/3u1uuj7l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20200925_183849-d37x7sgx\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m01_mf\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sf-moody/simple_mf\" target=\"_blank\">https://wandb.ai/sf-moody/simple_mf</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sf-moody/simple_mf/runs/d37x7sgx\" target=\"_blank\">https://wandb.ai/sf-moody/simple_mf/runs/d37x7sgx</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type      | Params\n",
      "-----------------------------------\n",
      "0 | user | Embedding | 30 K  \n",
      "1 | item | Embedding | 19 K  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac77d7f76584c10a80d6579db7a63b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Saving latest checkpoint..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b989521c7659473f87ee741acb61fbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'avg_test_loss': tensor(15.5915, device='cuda:0'),\n",
      " 'val_loss': tensor(15.5915, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.9486024339809935"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.test(model)\n",
    "mse = results['avg_test_loss']\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "01 Training a simple MF model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
