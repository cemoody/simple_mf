{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iA_qC5BAv6w"
   },
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to play around with this notebook, start by downloading the skipgram dataset from here:\n",
    "\n",
    "https://www.dropbox.com/s/nd1zxh538o6psal/skipgram_full.npz\n",
    "\n",
    "WARNING: it's a 1Gb download, so it may take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget  -q https://www.dropbox.com/s/nd1zxh538o6psal/skipgram_full.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "POjwTTneAv6y",
    "outputId": "b3acebb0-47b2-405c-eb40-5474b7aab5c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13835,    3257,    4605,  592814],\n",
       "       [  12071,    3257,      16,  491071],\n",
       "       [   4136,    3257,       2, -621270],\n",
       "       ...,\n",
       "       [  12293,    1390,       1, 1092727],\n",
       "       [   5103,    1390,       1, 2368132],\n",
       "       [   6789,    1390,       1,  427689]], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "codes = np.load(\"skipgram_full.npz\")['coded']\n",
    "# Remove duplicate skipgrams\n",
    "codes = codes[codes[:, 0] != codes[:, 1]]\n",
    "code2token = np.load(\"skipgram_full.npz\", allow_pickle=True)['c2t'].tolist()\n",
    "token2code = np.load(\"skipgram_full.npz\", allow_pickle=True)['t2c'].tolist()\n",
    "\n",
    "# First column is the first token code\n",
    "# second column is the 2nd token code\n",
    "# third column is the skip gram count\n",
    "# fourth is PMI * 1e6\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.592814,  0.491071, -0.62127 , ...,  1.092727,  2.368132,\n",
       "         0.427689], dtype=float32),\n",
       " 12.09618)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = codes[:, :2].copy().astype(np.int64)\n",
    "train_y = codes[:, 3].astype(np.float32) / 1e6\n",
    "train_y, train_y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['norris', 'roundhouse'],\n",
       " ['palpatine', 'skywalker'],\n",
       " ['palpatine', 'sith'],\n",
       " ['roundhouse', 'norris'],\n",
       " ['lankan', 'sri'],\n",
       " ['palpatine', 'anakin'],\n",
       " ['skywalker', 'palpatine'],\n",
       " ['anakin', 'palpatine'],\n",
       " ['blahblah', 'blah'],\n",
       " ['blah', 'blahblah']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_codes = np.argsort(train_y)[-10:]\n",
    "[[code2token[c[0]], code2token[c[1]]] for c in codes[top_codes, :2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_user = np.max(train_x[:, :2]) + 1\n",
    "n_item = np.max(train_x[:, :2]) + 1\n",
    "n_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from abstract_model import AbstractModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "def l2_regularize(array):\n",
    "    loss = torch.sum(array ** 2.0)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class MF(AbstractModel):\n",
    "    def __init__(self, n_user, n_item, train_x, train_y, test_x, test_y, \n",
    "                 k=128, c_vector=1.0, c_bias=1.0, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.save_data(train_x, train_y, test_x, test_y)\n",
    "        # These are simple hyperparameters\n",
    "        self.k = k\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.c_vector = c_vector\n",
    "        self.c_bias = c_bias\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # These are learned and fit by PyTorch\n",
    "        self.user = nn.Embedding(n_user, k)\n",
    "        self.item = nn.Embedding(n_item, k)\n",
    "        \n",
    "        # We've added new terms here:\n",
    "        self.bias_user = nn.Embedding(n_user, 1)\n",
    "        self.bias_item = nn.Embedding(n_item, 1)\n",
    "        self.bias = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # This is the most import function in this script\n",
    "        # These are the user indices, and correspond to \"u\" variable\n",
    "        user_id = inputs[:, 0]\n",
    "        # Item indices, correspond to the \"i\" variable\n",
    "        item_id = inputs[:, 1]\n",
    "        # vector user = p_u\n",
    "        vector_user = self.user(user_id)\n",
    "        # vector item = q_i\n",
    "        vector_item = self.item(item_id)\n",
    "        # this is a dot product & a user-item interaction: p_u * q_i\n",
    "        ui_interaction = torch.sum(vector_user * vector_item, dim=1)\n",
    "        \n",
    "        # Pull out biases\n",
    "        bias_user = self.bias_user(user_id).squeeze()\n",
    "        bias_item = self.bias_item(item_id).squeeze()\n",
    "        biases = (self.bias + bias_user + bias_item)\n",
    "\n",
    "        # Add bias prediction to the interaction prediction\n",
    "        prediction = ui_interaction + biases\n",
    "        return prediction\n",
    "    \n",
    "    def likelihood(self, prediction, target):\n",
    "        # MSE error between target = R_ui and prediction = p_u * q_i\n",
    "        loss_mse = F.mse_loss(prediction, target.squeeze())\n",
    "        return loss_mse\n",
    "    \n",
    "    def prior(self):\n",
    "        # Add new regularization to the biases\n",
    "        prior_bias_user =  l2_regularize(self.bias_user.weight) * self.c_bias\n",
    "        prior_bias_item = l2_regularize(self.bias_item.weight) * self.c_bias\n",
    "        # Compute L2 reularization over user (P) and item (Q) matrices\n",
    "        prior_user =  l2_regularize(self.user.weight) * self.c_vector\n",
    "        prior_item = l2_regularize(self.item.weight) * self.c_vector\n",
    "        # Add up the MSE loss + user & item regularization\n",
    "        total = prior_user + prior_item + prior_bias_user + prior_bias_item\n",
    "        return total\n",
    "\n",
    "\n",
    "#  For an unsupervised model, we're not  interested\n",
    "# in  the holdout metrics, so there is no  test set\n",
    "model = MF(n_user, n_item, train_x, train_y, train_x, train_y)\n",
    "\n",
    "# add a logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"w2v_model\")\n",
    "\n",
    "# We could have turned on multiple GPUs here, for example\n",
    "# trainer = pl.Trainer(gpus=8, precision=16)    \n",
    "trainer = pl.Trainer(max_epochs=5,\n",
    "                     reload_dataloaders_every_epoch=True,\n",
    "                     logger=logger)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AgnqWgCAv7H"
   },
   "source": [
    "#### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "oLvb4afZAv7F",
    "outputId": "c26239cd-63f2-4337-9a84-79aafbce46ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | user      | Embedding | 1 M   \n",
      "1 | item      | Embedding | 1 M   \n",
      "2 | bias_user | Embedding | 14 K  \n",
      "3 | bias_item | Embedding | 14 K  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaef0e4973eb4fa8a7c1fb2203804ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_05_word2vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_raw = model.user.weight.data.numpy()\n",
    "vectors = vectors_raw / np.sqrt((vectors_raw**2.0).sum(axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vectors[0]**2.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(token, n=10):\n",
    "    code = token2code[token]\n",
    "    vector = vectors[code]\n",
    "    similarity = np.sum(vector[None, :] * vectors, axis=1)\n",
    "    closest = np.argsort(similarity)[::-1]\n",
    "    for code in closest[1:n]:\n",
    "        print(code2token[code], similarity[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('hipster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('groovy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('bromance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('barbie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('pope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest('selfie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract and add word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subtract(center, minus, plus, n=10):\n",
    "    vector = (vectors[token2code[center]]\n",
    "             - vectors[token2code[minus]]\n",
    "             + vectors[token2code[plus]])\n",
    "    similarity = np.sum(vector[None, :] * vectors, axis=1)\n",
    "    closest = np.argsort(similarity)[::-1]\n",
    "    for code in closest[2:n]:\n",
    "        tok = code2token[code]\n",
    "        if tok != center and tok != minus and tok != plus:\n",
    "            print(code2token[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_subtract('burrito', 'mexican', 'italian')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "01 Training a simple MF model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
